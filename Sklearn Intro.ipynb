{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to sklearn and the experimental setup\n",
    "\n",
    "Scikit-Learn (http://scikit-learn.org) is a machine learning library in Python. It offers a wide range of tools for data mining and analysis. \n",
    "\n",
    "![alt text][logo]\n",
    "[logo]: http://scikit-learn.org/stable/_static/ml_map.png \"Sklearn Flowchart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Tools provided by sklearn:\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "3. Model Selection and Evaluation\n",
    "4. Dataset Transformations\n",
    "5. Dataset loading utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Data\n",
    "How you load your data is highly dependent on the type of data you are working with. Libraries like sklearn, nltk, etc. provide toy datasets as well as some functionality for loading data. For example sklearn provides functions for loading datasets in the svmlight format: \n",
    "\n",
    " &lt;label&gt; &lt;feature-id&gt;:&lt;feature-value&gt; &lt;feature-id&gt;:&lt;feature-value&gt; ... \n",
    " \n",
    "Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:\n",
    "\n",
    "- pandas.io provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides tools for manipulation and conversion into a numeric array suitable for scikit-learn.\n",
    "- scipy.io specializes in binary formats often used in scientific computing context such as .mat and .arff\n",
    "- numpy/routines.io for standard loading of columnar data into numpy arrays\n",
    "- scikit-learn’s datasets.load_svmlight_file for the svmlight or libSVM sparse format\n",
    "- scikit-learn’s datasets.load_files for directories of text files where the name of each directory is the name of each category and each file inside of each directory corresponds to one sample from that category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('male', 'Aamir'),\n",
      "    ('male', 'Aaron'),\n",
      "    ('male', 'Abbey'),\n",
      "    ('male', 'Abbie'),\n",
      "    ('male', 'Abbot')]\n",
      "[   ('female', 'Zorine'),\n",
      "    ('female', 'Zsa Zsa'),\n",
      "    ('female', 'Zsazsa'),\n",
      "    ('female', 'Zulema'),\n",
      "    ('female', 'Zuzana')]\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "name_data = [('male', name) for name in nltk.corpus.names.words('male.txt')]\n",
    "name_data.extend(('female', name) for name in nltk.corpus.names.words('female.txt'))\n",
    "\n",
    "#print the data\n",
    "pp.pprint(name_data[:5])\n",
    "pp.pprint(name_data[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "In this step we perform some transformation on the raw data. For example if you work with Tweets it is common to replace the username with a '@name' token. Other preprocessing involves lowercasing the text, tokenizing, stemming, chunking, etc. For this step the NLTK (http://www.nltk.org/) provides a wide range of tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Extraction\n",
    "\n",
    "Here we extract the features for our data. Feautres should reflect properties of a datapoint which helps to distinguish the different classes. Especially in NLP tasks we use the feature extraction to tranforming the textual data into numerical features. These features are what we use as input to the machine learning procedures. Sklearn provides the sklearn.feature_extraction modlue which implements common feature extraction methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (' ', 0),\n",
      "    (\"'\", 1),\n",
      "    ('-', 2),\n",
      "    ('A', 3),\n",
      "    ('B', 4),\n",
      "    ('C', 5),\n",
      "    ('D', 6),\n",
      "    ('E', 7),\n",
      "    ('F', 8),\n",
      "    ('G', 9),\n",
      "    ('H', 10),\n",
      "    ('I', 11),\n",
      "    ('J', 12),\n",
      "    ('K', 13),\n",
      "    ('L', 14),\n",
      "    ('M', 15),\n",
      "    ('N', 16),\n",
      "    ('O', 17),\n",
      "    ('P', 18),\n",
      "    ('Q', 19),\n",
      "    ('R', 20),\n",
      "    ('S', 21),\n",
      "    ('T', 22),\n",
      "    ('U', 23),\n",
      "    ('V', 24),\n",
      "    ('W', 25),\n",
      "    ('X', 26),\n",
      "    ('Y', 27),\n",
      "    ('Z', 28),\n",
      "    ('a', 29),\n",
      "    ('b', 30),\n",
      "    ('c', 31),\n",
      "    ('d', 32),\n",
      "    ('e', 33),\n",
      "    ('f', 34),\n",
      "    ('g', 35),\n",
      "    ('h', 36),\n",
      "    ('i', 37),\n",
      "    ('j', 38),\n",
      "    ('k', 39),\n",
      "    ('l', 40),\n",
      "    ('m', 41),\n",
      "    ('n', 42),\n",
      "    ('o', 43),\n",
      "    ('p', 44),\n",
      "    ('q', 45),\n",
      "    ('r', 46),\n",
      "    ('s', 47),\n",
      "    ('t', 48),\n",
      "    ('u', 49),\n",
      "    ('v', 50),\n",
      "    ('w', 51),\n",
      "    ('x', 52),\n",
      "    ('y', 53),\n",
      "    ('z', 54)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#bag of characters, bag of character ngrams\n",
    "cv = CountVectorizer(analyzer='char', preprocessor=None, lowercase=False)\n",
    "\n",
    "corpus = [d[1] for d in name_data]\n",
    "\n",
    "x_bow = cv.fit_transform(corpus)\n",
    "pp.pprint(sorted(cv.vocabulary_.items(),key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_bow)) #sparse matrix\n",
    "x_bow = x_bow.toarray() #make it dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7944,)\n",
      "(7944, 1)\n"
     ]
    }
   ],
   "source": [
    "#generate new features: length of the name\n",
    "x_nlengths = np.array([len(name) for name in corpus])\n",
    "print(x_nlengths.shape)\n",
    "#produces vector of shape (7944,) -> cannot be concatenated, transform into (7944,1) vector\n",
    "x_nlengths = np.expand_dims(x_nlengths, axis=-1)\n",
    "print(x_nlengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7944,)\n",
      "(7944, 1)\n"
     ]
    }
   ],
   "source": [
    "#generate new features: length of the name\n",
    "x_a = np.array([int(name[-1] == 'a') for name in corpus])\n",
    "print(x_a.shape)\n",
    "#produces vector of shape (7944,) -> cannot be concatenated, transform into (7944,1) vector\n",
    "x_a = np.expand_dims(x_a, axis=-1)\n",
    "print(x_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7944, 57)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine features\n",
    "X = np.concatenate([x_bow,x_nlengths,x_a], axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7944,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_label_voc = {\n",
    "    'male': 0,\n",
    "    'female': 1\n",
    "}\n",
    "Y = np.array([numeric_label_voc.get(d[0]) for d in name_data])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(C = female) = 0.6295317220543807\n",
      "Counter({'female': 5001, 'male': 2943})\n",
      "Counter({0: 6142, 1: 1802})\n",
      "Counter({(0, 'female'): 3228, (0, 'male'): 2914, (1, 'female'): 1773, (1, 'male'): 29})\n",
      "[('female', 'Bamby'), ('female', 'Brunhilde'), ('female', 'Corinne'), ('male', 'Pascal'), ('female', 'Kaia'), ('male', 'Warde'), ('female', 'Aphrodite'), ('female', 'Sophronia'), ('female', 'Perrine'), ('female', 'Elberta'), ('female', 'Katina'), ('female', 'Shirline'), ('female', 'Ciara'), ('female', 'Justine'), ('male', 'Nestor'), ('female', 'Carleen'), ('female', 'Corrine'), ('female', 'Daloris'), ('female', 'Gracia'), ('male', 'Piggy')]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "def get_sample(data, n):\n",
    "    return random.sample(data, n)\n",
    "\n",
    "labels = [d[0] for d in name_data]\n",
    "is_last_a = [int(name[-1] == 'a') for name in corpus]\n",
    "\n",
    "print('P(C = female) =', collections.Counter(labels)['female']/len(labels))\n",
    "\n",
    "print(collections.Counter(labels))\n",
    "print(collections.Counter(is_last_a))\n",
    "print(collections.Counter(zip(is_last_a, labels)))\n",
    "print(get_sample(name_data, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Choose a Classifier\n",
    "\n",
    "Goto http://scikit-learn.org/stable/supervised_learning.html and choose a classifier. Use the flowchart above to guide your decision. Check if the results align with the Flowchart.\n",
    "\n",
    "Just selecting a model and fitting it on the data is not enough. There are a couple pf problems we need to address:\n",
    "1) We need to know how well the trained model performs on unseen data. \n",
    "2) We need to select the hyper-parameters which yield the best model.\n",
    "\n",
    "In order to assess how well the trained model performs we apply K-Fold Cross Validation (CV). In K-Fold CV we partition the data into K equal sized subsamples. Of these K subsamples we keep one sample as the validation data and fit the model on the other K - 1 subsamples. This process is repreated K times until each subsample has been used as validation data.\n",
    "CV is also used to avoid overfitting, i.e. when the model corresponds too closely to the data it was fitted on. When the model overfits it reduces the generalization performance of the model. \n",
    "Now we just need to select the best suited metric to evlaute our model, there are lots of metrics you can choose from: accuracy, precision, recall, f1-score, mean squared error, ... \n",
    "\n",
    "For selecting the hyperparameters we use a technique called Grid Search. Grid Search tries all combinations of hyper-parameters and returns the CV-score for each combination. For each hyper-parameter you need to specifiy which values the Grid-Search should test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5229582811002241 0.13201772094230707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "scores = cross_val_score(clf, X, Y, cv=10, scoring='accuracy')\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   11.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True, 'max_depth': 1, 'max_features': 'log2', 'n_estimators': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [1, 2, 3, 4, 5, 10, 20, 50],\n",
    "    'max_features': [None, 'log2', 'sqrt'],\n",
    "    'max_depth': [None, 1, 2, 25],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=1, cv=5, scoring='accuracy')\n",
    "gs.fit(X,Y)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6340634441087614"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ki2_fs2018]",
   "language": "python",
   "name": "conda-env-ki2_fs2018-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
