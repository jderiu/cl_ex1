{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words: 31230\n",
      "Number of distinct words: 24375\n",
      "Number of distinct words: 16495\n",
      "Number of distinct words: 21809\n",
      "['social', 'verbreitet', 'Fluglinien', 'eintrat', 'Ziel-1-Regionen']\n",
      "['clausola', 'Nobel', 'rivelare', 'cattura', 'Lannoye']\n",
      "['places', 'foreseeing', 'lobbies', 'Employers', 'delighted']\n",
      "['contribution', 'délit', 'attenant', 'réponde', 'appropriation']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "from nltk.corpus.europarl_raw import german, italian, english, french\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "def print_ndistinct_words(word_list):\n",
    "    print('Number of distinct words: {}'.format(len(set(word_list))))\n",
    "\n",
    "    \n",
    "def sample_data(data, n):\n",
    "    \"\"\"\n",
    "    Return random sample of data of size n\n",
    "\n",
    "    \"\"\"\n",
    "    return random.sample(data, n)\n",
    "#show lengths\n",
    "print_ndistinct_words(german.words())\n",
    "print_ndistinct_words(italian.words())\n",
    "print_ndistinct_words(english.words())\n",
    "print_ndistinct_words(french.words())\n",
    "\n",
    "#get unique words\n",
    "german_words = set(german.words())\n",
    "italian_words = set(italian.words())\n",
    "english_words = set(english.words())\n",
    "french_words = set(french.words())\n",
    "\n",
    "#show a bit of data\n",
    "pp.pprint(sample_data(german_words, 5))\n",
    "pp.pprint(sample_data(italian_words, 5))\n",
    "pp.pprint(sample_data(english_words, 5))\n",
    "pp.pprint(sample_data(french_words, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93909\n",
      "[   (2, 'attributed'),\n",
      "    (0, 'Mißstände'),\n",
      "    (3, '13h20'),\n",
      "    (1, 'purificano'),\n",
      "    (2, 'moment'),\n",
      "    (2, 'micro-organisms'),\n",
      "    (0, 'Barnhill'),\n",
      "    (2, 'C5-0333'),\n",
      "    (3, 'due'),\n",
      "    (0, 'Arbeitskraft'),\n",
      "    (2, 'customary'),\n",
      "    (1, 'long'),\n",
      "    (0, 'maximieren'),\n",
      "    (1, 'patti'),\n",
      "    (1, 'deturpare'),\n",
      "    (3, 'Cuba'),\n",
      "    (2, 'monopolies'),\n",
      "    (0, 'Agrarabschöpfung'),\n",
      "    (0, 'entscheidender'),\n",
      "    (0, 'Gefräßigkeit')]\n"
     ]
    }
   ],
   "source": [
    "#prepare labelled dataset\n",
    "german_data = [(0, token) for token in german_words]\n",
    "italian_data = [(1, token) for token in italian_words]\n",
    "english_data = [(2, token) for token in english_words]\n",
    "french_data = [(3, token) for token in french_words]\n",
    "\n",
    "full_data = german_data + italian_data + english_data + french_data\n",
    "print(len(full_data))\n",
    "pp.pprint(sample_data(full_data, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(93909, 16006)\n",
      "(93909,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#bag of characters, bag of character ngrams\n",
    "cv = CountVectorizer(analyzer='char', preprocessor=None, lowercase=False, ngram_range=(2,3))\n",
    "\n",
    "corpus = [d[1] for d in full_data]\n",
    "\n",
    "X = cv.fit_transform(corpus)\n",
    "print(type(X)) #sparse matrix\n",
    "print(X.shape)\n",
    "\n",
    "Y = np.array([d[0] for d in full_data])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74495307 0.73835364 0.74086496]\n",
      "0.7413905590052815 0.002719716973797653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, X, Y, cv=3, scoring='f1_macro')\n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ki2_fs2018]",
   "language": "python",
   "name": "conda-env-ki2_fs2018-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
