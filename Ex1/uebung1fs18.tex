\documentclass[a4paper,10pt]{article}
\usepackage[headings]{fullpage}

\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{url}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage[osf,sc]{mathpazo}
\usepackage{graphicx}
\usepackage{fancyhdr}
\fancypagestyle{plain}{}
\pagestyle{fancy}
\fancyhf{}

\lstset{
  language=Python,
  basicstyle = \small\ttfamily,
  keywordstyle = \color{blue},
  commentstyle = \itshape\color{red},
  stringstyle = \color{gray},
%  otherkeywords = {>>>},
  columns = flexible,
  showstringspaces = false,
  mathescape = false,
  inputencoding = utf8x,
  extendedchars = \true
}

\newcommand{\punkte}[1]{(\emph{#1 p})}

\newcommand{\argmax}{\operatornamewithlimits{argmax}} 
\newcommand{\argmin}{\operatornamewithlimits{argmin}} 

\fancyhead[L]{\small Maschinelle Lernverfahren für die Sprachtechnologie (FS 16)\\Simon Clematide}
\fancyhead[R]{\small 2. März 2016\\Institut für Computerlinguistik, UZH}

\title{Übungen 1: Informationstheorie und  Klassifikation}
%\author{Aufgabenstellung: Simon \texttt{<simon.clematide@cl.uzh.ch>} }
\date{\textbf{Abgabedatum} Sonntag, 18. März 2016, 13:00h \textsc{mez}
}
\begin{document}
\maketitle

\section*{Setup}
Installiere Anaconda für Python 3.6 wie auf der Web-Site\footnote{https://www.anaconda.com/download/} beschrieben. Um die benötigten Packages zu installieren, führe im \emph{Terminal} oder in der \emph{Eingabeaufforderung} folgenden Befehl aus: 
\begin{lstlisting}[keepspaces=true,basicstyle=\ttfamily\footnotesize,language={}]
conda install nb_conda
conda env create -f ki2_fs2018_env.yml
\end{lstlisting}
Um die Daten nutzen zu können, müssen wir diese über das NLTK-Toolkit herunterladen. Führe dazu folgende Befehle aus:
\begin{lstlisting}[keepspaces=true,basicstyle=\ttfamily\footnotesize,language={}]
$ python
>>> import nltk
>>> nltk.download("names")
>>> nltk.download("europarl_raw")
\end{lstlisting}
Um das Notebook zu starten gehe im Terminal zum Pfad in dem das \emph{Language Classification.ipynb} und das \emph{Sklearn Intro.ipynb} Notebook gespeichert sind und führe folgenden Befehl aus:
\begin{lstlisting}[keepspaces=true,basicstyle=\ttfamily\footnotesize,language={}]
jupyter notebook
\end{lstlisting}

\section{Maximum-Likelihood-Modelle und Mutual Information}
%\punkte{20}
In dieser Aufgabe testest du mögliche Merkmale auf ihren erwarteten Informationsgewinn bezüglich einer binären Klassifikation.
Als Beispiel dient uns die Geschlechtsklassifikation für englische Vornamen, welche du in der Vorlesung kennen gelernt hast. 
Die \textbf{Zufallsvariable C} mit den beiden möglichen Ergebnissen $\Omega =\{male,female\}$ repräsentiert die beiden zulässigen \textbf{Klassen} der Vornamen.
Die boolesche \textbf{Zufallsvariable F} ($\Omega = \{True,False\}$) ist wahr, wenn ein Vorname auf -a endet und sonst falsch. Dieses Merkmal F ist vordefiniert mit Hilfe der Merkmalsextraktionsfunktion \lstinline!feature(Vorname)!. 

\subsection{Verstehen der Kodevorlage}
%\punkte{5}

\begin{enumerate}
\item In den Vorlesungsunterlagen findest du die Datei \texttt{Sklearn Intro.ipynb}. Studiere den Quellcode und die Kommentare dazu. Falls du die Klasse \lstinline!collection.Counter! für Häufigkeitsverteilungen nicht kennst, studiere deren offizielle Dokumentation\footnote{\url{https://docs.python.org/2/library/collections.html\#collections.Counter}}. Falls du Listenkomprehension (\textit{list comprehension}) und Generatorausdrücke (\textit{generator expressions}) nicht kennst, studiere die entsprechenden Abschnitte im NLTK-Buch\footnote{Z.B. \url{http://www.nltk.org/book/ch04.html\#generator_expression_index_term}}.

\item Im Abschnitt 4 des Notebooks findest du den \emph{Data Analysis} Teil. Berechne aus den Häufigkeitsverteilungen (\textit{frequency distributions}) folgende Maximum-Likelihood-Wahrscheinlichkeiten sowohl für die Gesamtdaten als auch für ein zufällig gezogenes Sample von 500 Vornamen:
\begin{enumerate}
\item $p(C=female)$
\item $p(F=True)$
\item $p(C=female|F=True)$
\item $p(F=True|C=Female)$
\item $p(C=female,F=True)$
\end{enumerate}
Gib die entsprechenden Resultate mit einem Titel wie in der Vorlage mittel \lstinline!print! aus.
\end{enumerate}

\subsection{Entropie und Informationsgewinn implementieren}


\begin{enumerate}
\item Implementiere die Formel, welche die Entropie berechnet, und evaluiere sie für C:
\begin{equation}
H(X) = -\sum_{x \in X} {p(x)\log(p(x))}
\end{equation}
\item Selbsttest: Die Formel sollte $0.9510$ ergeben.
\item Implementiere die Mutual Information für $I(C;F)$ und evaluiere sie auf den Gesamtdaten und auf einem zufälligen Sample von 500 Vornamen:
\begin{equation}
I(X;Y) = \sum_{x \in X, y \in Y}{p(x,y) \log{\frac{p(x,y)}{p(x)p(y)}}}
\end{equation}\\ Hinweis: $0\log(0)$ soll im Kontext der Informationstheorie als $0$ definiert sein.
\\ Hinweis: \texttt{math.log} in Python berechnet standardmässig den natürlichen Logarithmus. Der Logarithmus mit Basis 2 wird mit dem optionalen Basis-Argument aufgerufen: \lstinline!math.log(8, 2)! evaluiert zu \lstinline!3.0!.
\item Selbsttest: Die Formel sollte $0.1524$ ergeben auf den Gesamtdaten.
\end{enumerate}



\section{Klassifikation und Experimentation }
Im Notebook \emph{Sklearn Intro.ipynb} wird der generelle Aufbau des \emph{Scikit-Learn}\footnote{http://scikit-learn.org} gezeigt. In dieser Aufgabe geht es darum selber mit dem Toolkit zu experimentieren. 
Im Notebook \emph{Language Classification.ipynb} findest du den rudimentären Aufbau eines Experiments um die Sprache einzelner Wörter zu klassifizieren. 
\begin{enumerate}
\item Studiere den Code und beschreibe kurz was der Code macht. 
\item Feature Extraction: Im Code wird der \emph{CountVectorizer} benutzt für die Feautre extraction. Lass den Code mit verschiedenen Parameterwahlen im \emph{CountVectorizer} laufen. Nutze drei verschiedene Werte \emph{ngram-range} und beschreibe die Resultate. Verwende auch nicht lower-case characters und beschreibe die Resultate. 
\item Model: Verwende verschiedene Algorithmen aus dem Sklearn Sortiment: Wähle basierend auf dem Chart im \emph{Sklearn Intro.ipynb} Notebook zwei weitere Algorthmen aus und schreib die Resultate auf.
\item Model Selection: Nutze \emph{GridSearchCV} um für einen der drei Algorithmen die beste Kombination an Hyperparameter zu finden. 
\end{enumerate}
\section{Feedback}
Bitte melde kurz zurück, welche Aufgaben du hilfreich und welche du nicht hilfreich fandest, um dein Lernen zu unterstützen.

\hfil Viel Spass und Erfolg beim Programmieren!
\end{document}
