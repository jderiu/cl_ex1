{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words: 31230\n",
      "Number of distinct words: 24375\n",
      "Number of distinct words: 16495\n",
      "Number of distinct words: 21809\n",
      "['stammenden', 'Zinsen', 'transparentes', 'Terroranschläge', '+']\n",
      "['caldo', 'irto', 'misto', 'kosovara', 'terzo']\n",
      "['team', 'Akkuyu', 'alleviation', 'peninsula', 'cosmopolitan']\n",
      "['ravages', 'lever', 'tenez', 'instituer', 'réinvesties']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "from nltk.corpus.europarl_raw import german, italian, english, french\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "def print_ndistinct_words(word_list):\n",
    "    print('Number of distinct words: {}'.format(len(set(word_list))))\n",
    "\n",
    "    \n",
    "def sample_data(data, n):\n",
    "    \"\"\"\n",
    "    Return random sample of data of size n\n",
    "\n",
    "    \"\"\"\n",
    "    return random.sample(data, n)\n",
    "#show lengths\n",
    "print_ndistinct_words(german.words())\n",
    "print_ndistinct_words(italian.words())\n",
    "print_ndistinct_words(english.words())\n",
    "print_ndistinct_words(french.words())\n",
    "\n",
    "#get unique words\n",
    "german_words = set(german.words())\n",
    "italian_words = set(italian.words())\n",
    "english_words = set(english.words())\n",
    "french_words = set(french.words())\n",
    "\n",
    "#show a bit of data\n",
    "pp.pprint(sample_data(german_words, 5))\n",
    "pp.pprint(sample_data(italian_words, 5))\n",
    "pp.pprint(sample_data(english_words, 5))\n",
    "pp.pprint(sample_data(french_words, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93909\n",
      "[   (0, 'EU-Perspektive'),\n",
      "    (1, 'alchimisti'),\n",
      "    (0, 'Velzen'),\n",
      "    (3, 'contraignent'),\n",
      "    (1, 'Bassa-Normandia'),\n",
      "    (1, 'Creatore'),\n",
      "    (1, 'sottoporgli'),\n",
      "    (0, 'gefoltert'),\n",
      "    (0, 'Personen'),\n",
      "    (1, 'incomprensibile'),\n",
      "    (1, 'acute'),\n",
      "    (2, 'repatriating'),\n",
      "    (0, 'Schäden'),\n",
      "    (3, 'Europe-Afrique'),\n",
      "    (1, 'conforme'),\n",
      "    (1, 'risposto'),\n",
      "    (3, 'dupés'),\n",
      "    (0, 'Transportbereich'),\n",
      "    (3, '350'),\n",
      "    (3, 'attestations')]\n"
     ]
    }
   ],
   "source": [
    "#prepare labelled dataset\n",
    "german_data = [(0, token) for token in german_words]\n",
    "italian_data = [(1, token) for token in italian_words]\n",
    "english_data = [(2, token) for token in english_words]\n",
    "french_data = [(3, token) for token in french_words]\n",
    "\n",
    "full_data = german_data + italian_data + english_data + french_data\n",
    "print(len(full_data))\n",
    "pp.pprint(sample_data(full_data, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(93909, 16006)\n",
      "(93909,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#bag of characters, bag of character ngrams\n",
    "cv = CountVectorizer(analyzer='char', preprocessor=None, lowercase=False, ngram_range=(2,3))\n",
    "\n",
    "corpus = [d[1] for d in full_data]\n",
    "\n",
    "X = cv.fit_transform(corpus)\n",
    "print(type(X)) #sparse matrix\n",
    "print(X.shape)\n",
    "\n",
    "Y = np.array([d[0] for d in full_data])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81025324 0.80848333 0.8060338 ]\n",
      "0.8082567890145214 0.0017300137990102621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_score(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "clf = LinearSVC()\n",
    "scores = cross_val_score(clf, X, Y, cv=3, scoring='f1_macro')\n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ki2_fs2018]",
   "language": "python",
   "name": "conda-env-ki2_fs2018-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
